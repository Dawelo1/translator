{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Dokumentacja projektu Translator","text":"<p>Witamy w dokumentacji projektu Translator \u2013 aplikacji s\u0142u\u017c\u0105cej do t\u0142umaczenia tekst\u00f3w z j\u0119zyka polskiego na inne j\u0119zyki przy u\u017cyciu modeli neuronowych (OpenNMT, Hugging Face).</p>"},{"location":"#zawartosc-dokumentacji","title":"Zawarto\u015b\u0107 dokumentacji","text":"<p>Znajdziesz tu:</p> <ul> <li>Podstawowe informacje o projekcie i jego celach</li> <li>Opis u\u017cytych danych oraz modeli t\u0142umaczeniowych</li> <li>Przegl\u0105d literatury i inspiracji</li> <li>Instrukcj\u0119 uruchomienia projektu (lokalnie i przez Dockera)</li> <li>Zrzuty ekranu dzia\u0142ania GUI i API</li> <li>Wnioski i mo\u017cliwe kierunki rozwoju</li> </ul>"},{"location":"#nawigacja","title":"Nawigacja","text":"<p>U\u017cyj menu po lewej stronie, aby przej\u015b\u0107 do interesuj\u0105cych Ci\u0119 sekcji dokumentacji.</p> <p>Repozytorium projektu: GitHub - Dawelo1/translator</p>"},{"location":"1_streszczenie/","title":"1. Streszczenie","text":"<p>Translator to aplikacja do t\u0142umaczenia tekst\u00f3w z j\u0119zyka polskiego na angielski z wykorzystaniem modeli neuronowych. Sk\u0142ada si\u0119 z backendu (API) oraz frontendowego interfejsu (GUI). W projekcie zastosowano modele <code>mbart-50</code> oraz <code>opus-mt</code> z biblioteki <code>transformers</code>.</p> <p>Celem jest stworzenie prostego i funkcjonalnego narz\u0119dzia t\u0142umaczeniowego, dost\u0119pnego lokalnie lub jako us\u0142uga webowa.</p>"},{"location":"2_wprowadzenie/","title":"2. Wprowadzenie","text":"<p>W ostatnich latach narz\u0119dzia do t\u0142umaczenia maszynowego oparte na g\u0142\u0119bokich sieciach neuronowych znacz\u0105co poprawi\u0142y jako\u015b\u0107 automatycznej translacji tekst\u00f3w. Projekt Translator powsta\u0142 jako pr\u00f3ba stworzenia samodzielnej aplikacji umo\u017cliwiaj\u0105cej t\u0142umaczenie offline z j\u0119zyka polskiego na angielski, bez potrzeby korzystania z komercyjnych us\u0142ug chmurowych takich jak Google Translate.</p>"},{"location":"2_wprowadzenie/#cel-projektu","title":"Cel projektu","text":"<p>G\u0142\u00f3wne cele projektu to:</p> <ul> <li>stworzenie aplikacji t\u0142umacz\u0105cej z polskiego na angielski (jednokierunkowo),</li> <li>por\u00f3wnanie dzia\u0142ania dw\u00f3ch gotowych modeli t\u0142umaczeniowych typu encoder-decoder:</li> <li><code>Helsinki-NLP/opus-mt-pl-en</code>,</li> <li><code>facebook/mbart-large-50-many-to-many-mmt</code>,</li> <li>zapewnienie prostego API oraz graficznego interfejsu GUI (Streamlit) do wykonywania t\u0142umacze\u0144,</li> <li>wdro\u017cenie aplikacji jako us\u0142ugi chmurowej w Microsoft Azure (z wykorzystaniem kontener\u00f3w Docker).</li> </ul> <p>Lokalne uruchomienie aplikacji s\u0142u\u017cy g\u0142\u00f3wnie jako \u015brodowisko testowe i deweloperskie. Finalnym celem projektu jest udost\u0119pnienie systemu jako us\u0142ugi webowej dla u\u017cytkownik\u00f3w zewn\u0119trznych.</p>"},{"location":"2_wprowadzenie/#modele-tumaczeniowe","title":"Modele t\u0142umaczeniowe","text":"<p>Projekt wykorzystuje dwa r\u00f3\u017cne podej\u015bcia do t\u0142umaczenia:</p> <ol> <li>Opus-MT (<code>opus-mt-pl-en</code>) \u2014 lekki, szybki model oparty na architekturze MarianMT, przeznaczony do prostych zastosowa\u0144 produkcyjnych.</li> <li>MBart-50 (<code>mbart-large-50-many-to-many-mmt</code>) \u2014 wi\u0119kszy, bardziej z\u0142o\u017cony model przystosowany do wielu j\u0119zyk\u00f3w, z lepszym rozumieniem kontekstu i wi\u0119ksz\u0105 dok\u0142adno\u015bci\u0105.</li> </ol> <p>Oba modele s\u0105 pretrenowane i dost\u0119pne za darmo poprzez platform\u0119 Hugging Face.</p>"},{"location":"2_wprowadzenie/#architektura-i-funkcjonalnosci","title":"Architektura i funkcjonalno\u015bci","text":"<p>G\u0142\u00f3wne komponenty implementacyjne:</p> <ul> <li> <p>Inicjalizacja modeli:   Modele s\u0105 \u0142adowane z Hugging Face i przygotowywane do dzia\u0142ania na GPU (je\u015bli dost\u0119pny) lub CPU. W przypadku <code>mbart-50</code> dodatkowo inicjalizowany jest tokenizer z ustawionym j\u0119zykiem \u017ar\u00f3d\u0142owym jako <code>pl_PL</code>.</p> </li> <li> <p>T\u0142umaczenie tekstu:   Tekst wej\u015bciowy dzielony jest na zdania za pomoc\u0105 NLTK (<code>sent_tokenize</code>) i t\u0142umaczony sekwencyjnie. Dla <code>opus-mt</code> ka\u017cde zdanie jest t\u0142umaczone niezale\u017cnie, natomiast <code>mbart-50</code> generuje t\u0142umaczenie dla ca\u0142ego bloku tekstu.</p> </li> <li> <p>Ewaluacja BLEU:   Funkcja <code>evaluate_models_on_dataset()</code> wykorzystuje bibliotek\u0119 <code>evaluate</code> i metryk\u0119 sacreBLEU, aby oceni\u0107 jako\u015b\u0107 t\u0142umacze\u0144 w odniesieniu do zestawu referencyjnych t\u0142umacze\u0144. Oba modele s\u0105 testowane na tych samych danych wej\u015bciowych i wynik por\u00f3wnywany.</p> </li> </ul>"},{"location":"2_wprowadzenie/#wykorzystane-biblioteki","title":"Wykorzystane biblioteki","text":"<ul> <li><code>transformers</code> \u2013 obs\u0142uga modeli <code>opus-mt</code> i <code>mbart-50</code>,</li> <li><code>nltk</code> \u2013 tokenizacja zda\u0144,</li> <li><code>evaluate</code> \u2013 ewaluacja jako\u015bci t\u0142umacze\u0144 za pomoc\u0105 metryk (BLEU),</li> <li><code>torch</code> \u2013 obs\u0142uga GPU/CPU i dzia\u0142ania modeli PyTorch,</li> <li><code>pipeline</code> \u2013 uproszczone API dla translacji.</li> </ul> <p>W kolejnych sekcjach znajdziesz szczeg\u00f3\u0142owy opis danych, metody uruchamiania aplikacji, proces wdro\u017cenia oraz wyniki por\u00f3wnania jako\u015bci t\u0142umacze\u0144.</p>"},{"location":"3_literatura/","title":"3. Przegl\u0105d literatury","text":"<p>Projekt oparty jest na modelu t\u0142umaczeniowym OpenNMT i korzysta z gotowych modeli dost\u0119pnych przez Hugging Face.</p> <p>\u0179r\u00f3d\u0142a:</p> <ul> <li> <p>Klein et al. (2017), \u201cOpenNMT: Open-Source Toolkit for Neural Machine Translation\u201d</p> </li> <li> <p>https://huggingface.co/Helsinki-NLP/opus-mt</p> </li> <li> <p>Dokumentacja PyTorch, Hugging Face Transformers</p> </li> </ul>"},{"location":"4_dane/","title":"4. Opis danych","text":"<p>Dane wej\u015bciowe: - Tekst w j\u0119zyku polskim (mog\u0105 to by\u0107 pojedyncze zdania lub d\u0142u\u017csze fragmenty). - Mo\u017cliwo\u015b\u0107 wczytania plik\u00f3w tekstowych do przetwarzania.</p> <p>Dane wyj\u015bciowe: - T\u0142umaczenia wy\u0142\u0105cznie na j\u0119zyk angielski.</p> <p>W projekcie wykorzystujemy dwa gotowe, wytrenowane wcze\u015bniej modele t\u0142umaczeniowe: <code>mbart-50</code> i <code>opus-mt</code>. Modele te zosta\u0142y opracowane odpowiednio przez Facebook AI (obecnie Meta AI) oraz zesp\u00f3\u0142 Helsinki-NLP i by\u0142y trenowane na du\u017cych, wieloj\u0119zycznych korpusach takich jak Europarl i Tatoeba.</p> <p>Por\u00f3wnujemy jako\u015bci t\u0142umacze\u0144 generowanych przez te modele na wsp\u00f3lnym zbiorze danych wej\u015bciowych.</p>"},{"location":"5_wdrozenie/","title":"Cykl wdra\u017cania aplikacji z wykorzystaniem obraz\u00f3w Dockerowych na platformie Microsoft Azure","text":""},{"location":"5_wdrozenie/#1-przygotowanie-srodowiska-aplikacji","title":"1. Przygotowanie \u015brodowiska aplikacji","text":"<p>Proces wdra\u017cania rozpoczyna si\u0119 od przygotowania dw\u00f3ch cz\u0119\u015bci aplikacji:</p> <ul> <li>Backend (API) \u2014 odpowiedzialny za logik\u0119, obs\u0142ug\u0119 \u017c\u0105da\u0144 u\u017cytkownik\u00f3w i testy.  </li> <li>Frontend (interfejs u\u017cytkownika) \u2014 odpowiadaj\u0105cy za wy\u015bwietlanie tre\u015bci i interakcje u\u017cytkownika.</li> </ul> <p>Dla obu cz\u0119\u015bci tworzy si\u0119 osobne obrazy Docker, kt\u00f3re zawieraj\u0105 wszystkie niezb\u0119dne zale\u017cno\u015bci i konfiguracj\u0119. Dzi\u0119ki temu uzyskujemy przeno\u015bne \u015brodowisko uruchomieniowe niezale\u017cne od systemu operacyjnego. Obrazy s\u0105 nast\u0119pnie wypychane do Docker Hub.</p>"},{"location":"5_wdrozenie/#2-przechowywanie-obrazow-kontenerowych","title":"2. Przechowywanie obraz\u00f3w kontenerowych","text":"<p>Obrazy Dockerowe s\u0105 przechowywane w Docker Hub \u2014 publicznym lub prywatnym rejestrze kontener\u00f3w. Docker Hub umo\u017cliwia \u0142atwe udost\u0119pnianie i wersjonowanie obraz\u00f3w.</p> <p>Po ich przygotowaniu lokalnie, przesy\u0142a si\u0119 je do Docker Hub, sk\u0105d s\u0105 pobierane przez us\u0142ugi w chmurze Microsoft Azure, takie jak Azure Web App for Containers, umo\u017cliwiaj\u0105ce bezpo\u015brednie wdra\u017canie aplikacji z u\u017cyciem wskazanych obraz\u00f3w.</p>"},{"location":"5_wdrozenie/#3-konfiguracja-usugi-azure-web-app-for-containers","title":"3. Konfiguracja us\u0142ugi Azure Web App for Containers","text":"<p>Do uruchomienia aplikacji bez zarz\u0105dzania infrastruktur\u0105 s\u0142u\u017cy Azure Web App for Containers. Tworzone s\u0105 dwa niezale\u017cne Web Appy:</p> <ul> <li>Web App dla API \u2014 obs\u0142uguje logik\u0119 aplikacji oraz \u017c\u0105dania HTTP/HTTPS.</li> <li>Web App dla frontendu \u2014 serwuje interfejs u\u017cytkownika jako aplikacj\u0119 webow\u0105.</li> </ul> <p>Podczas konfiguracji wskazuje si\u0119 nazw\u0119 obrazu i jego wersj\u0119 (tag) z Docker Huba. Obie aplikacje s\u0105 publicznie dost\u0119pne, co upraszcza konfiguracj\u0119 i minimalizuje zu\u017cycie zasob\u00f3w.</p>"},{"location":"5_wdrozenie/#4-konfiguracja-srodowiska-aplikacji","title":"4. Konfiguracja \u015brodowiska aplikacji","text":"<p>Ustawiane s\u0105 zmienne \u015brodowiskowe, kt\u00f3re umo\u017cliwiaj\u0105 poprawne dzia\u0142anie aplikacji, m.in.:</p> <ul> <li>adresy URL do us\u0142ug (np. baza danych),</li> <li>klucze API,</li> <li>\u015bcie\u017cki dost\u0119pu.</li> </ul> <p>Pozwala to zarz\u0105dza\u0107 konfiguracj\u0105 bez ingerencji w kod \u017ar\u00f3d\u0142owy.</p>"},{"location":"5_wdrozenie/#5-udostepnienie-aplikacji-uzytkownikom","title":"5. Udost\u0119pnienie aplikacji u\u017cytkownikom","text":"<p>Po wdro\u017ceniu aplikacja staje si\u0119 dost\u0119pna pod publicznymi adresami URL. Azure umo\u017cliwia r\u00f3wnie\u017c:</p> <ul> <li>przypisanie w\u0142asnej domeny,</li> <li>aktywacj\u0119 certyfikat\u00f3w SSL (HTTPS).</li> </ul> <p>W projekcie nie zastosowano tej opcji z uwagi na ograniczenia subskrypcji.</p>"},{"location":"5_wdrozenie/#6-problemy-wynikajace-z-subskrypcji-na-azure","title":"6. Problemy wynikaj\u0105ce z subskrypcji na Azure","text":"<p>W zwi\u0105zku z ograniczon\u0105 subskrypcj\u0105 Azure for Students, aplikacja nie mo\u017ce by\u0107 uruchomiona na sta\u0142e.</p> <ul> <li>Obraz API wa\u017cy oko\u0142o 6 GB, ze wzgl\u0119du na wbudowane modele j\u0119zykowe.</li> <li>Po godzinie bezczynno\u015bci aplikacje s\u0105 automatycznie zatrzymywane.</li> <li>Uruchomienie back-endu trwa d\u0142ugo, co wp\u0142ywa na wygod\u0119 u\u017cytkowania.</li> </ul>"},{"location":"6_wyniki/","title":"6. Wyniki \u2013 zrzuty ekranu","text":""},{"location":"6_wyniki/#gui-interfejs-uzytkownika","title":"GUI \u2013 Interfejs u\u017cytkownika","text":""},{"location":"6_wyniki/#api-odpowiedz-na-zapytanie","title":"API \u2013 Odpowied\u017a na zapytanie","text":""},{"location":"6_wyniki/#przykady","title":"Przyk\u0142ady","text":""},{"location":"7_wnioski/","title":"7. Wnioski","text":"<p>Projekt Translator potwierdza, \u017ce t\u0142umaczenie tekst\u00f3w z j\u0119zyka polskiego na angielski mo\u017ce by\u0107 skutecznie realizowane lokalnie przy u\u017cyciu gotowych modeli neuronowych, bez potrzeby korzystania z komercyjnych us\u0142ug chmurowych.</p> <p>Najwa\u017cniejsze wnioski: - Aplikacja dzia\u0142a stabilnie i wydajnie, a dzi\u0119ki konteneryzacji z u\u017cyciem Dockera mo\u017cliwe jest szybkie uruchomienie zar\u00f3wno lokalnie, jak i w chmurze (np. Microsoft Azure). - Zastosowano dwa gotowe, otwarte modele t\u0142umaczeniowe:   - Opus-MT (<code>Helsinki-NLP/opus-mt-pl-en</code>) \u2014 lekki i szybki model, bardzo dobrze radzi sobie z prostymi, technicznymi tekstami. Idealny do zastosowa\u0144 w \u015brodowiskach o ograniczonych zasobach.   - mBART-50 (<code>facebook/mbart-large-50-many-to-many-mmt</code>) \u2014 wi\u0119kszy i bardziej zaawansowany model, oparty na architekturze seq2seq z pretreningiem. Zdecydowanie lepiej radzi sobie z kontekstem, niuansami j\u0119zykowymi i bardziej naturalnym stylem t\u0142umaczenia, kosztem wi\u0119kszego zapotrzebowania na zasoby obliczeniowe.</p> <ul> <li>W bezpo\u015brednim por\u00f3wnaniu:  </li> <li>Opus-MT oferuje szybsze odpowiedzi i mniejsze zu\u017cycie pami\u0119ci.  </li> <li> <p>mBART-50 generuje bardziej naturalne i p\u0142ynne t\u0142umaczenia, szczeg\u00f3lnie przy d\u0142u\u017cszych i bardziej z\u0142o\u017conych tekstach.</p> </li> <li> <p>Mo\u017cliwo\u015bci dalszego rozwoju obejmuj\u0105:  </p> </li> <li>dodanie opcji logowania i oceny wynik\u00f3w,</li> <li>rozbudow\u0119 interfejsu graficznego,</li> <li>integracj\u0119 z bazami danych lub platformami edukacyjnymi.</li> </ul> <p>Projekt ma warto\u015b\u0107 edukacyjn\u0105 oraz potencja\u0142 do praktycznego wykorzystania jako lekkie narz\u0119dzie do lokalnego t\u0142umaczenia tekst\u00f3w specjalistycznych lub prywatnych.</p>"}]}